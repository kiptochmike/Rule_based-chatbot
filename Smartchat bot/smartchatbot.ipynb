{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the corpus of text\n",
    "f = open('text.txt', 'r', errors='ignore')\n",
    "raw_doc = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc = raw_doc.lower()  # lowercase\n",
    "nltk.download('punkt')  # punk tokenizer\n",
    "nltk.download('wordnet')  # wordnet dictionary\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nmain menu\\n\\nwikipediathe free encyclopedia\\nsearch wikipedia\\nsearch\\ncreate account\\nlog in\\n\\npersonal tools\\ncontents hide\\n(top)\\nbackground\\ndevelopment\\napplication\\ntoggle application subsection\\nmessaging apps\\nas part of company apps and websites\\nchatbot sequences\\ncompany internal platforms\\ncustomer service\\nhealthcare\\npolitics\\ngovernment\\ntoys\\nmalicious use\\ndata security\\nlimitations of chatbots\\nchatbots and jobs\\nsee also\\nreferences\\nfurther reading\\nexternal links\\nchatbot\\n\\narticle\\ntalk\\nread\\nedit\\nview history\\n\\ntools\\nappearance hide\\ntext\\n\\nsmall\\n\\nstandard\\n\\nlarge\\nwidth\\n\\nstandard\\n\\nwide\\ncolor (beta)\\n\\nautomatic\\n\\nlight\\n\\ndark\\nreport an issue with dark mode\\nfrom wikipedia, the free encyclopedia\\nfor the bot-creation software, see chatbot.',\n",
       " 'for bots on internet relay chat, see irc bot.',\n",
       " 'parts of this article (those related to everything, particularly sections after the intro) need to be updated.',\n",
       " 'the reason given is: this article is using citations from 1970 and virtually all claims about conversational capabilities are at least ten years out of date (for example the turing test was arguably made obsolete years ago by transformer models).',\n",
       " 'please help update this article to reflect recent events or newly available information.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main',\n",
       " 'menu',\n",
       " 'wikipediathe',\n",
       " 'free',\n",
       " 'encyclopedia',\n",
       " 'search',\n",
       " 'wikipedia',\n",
       " 'search',\n",
       " 'create',\n",
       " 'account']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greeting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = (\"hello\", \"hi\", \"whatsapp\", \"how are you?\")\n",
    "greet_responses = (\"hi\", \"hey\", \"hey there!\", \"there there!!\")\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo1_response = ''\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    all_sentences = sentence_tokens + [user_response]\n",
    "    tfidf = TfidfVec.fit_transform(all_sentences)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])\n",
    "    \n",
    "    # Find the most similar sentence\n",
    "    idx = vals.argsort()[0][-1]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-1]\n",
    "    \n",
    "    # Check if the similarity score is above a threshold\n",
    "    if req_tfidf == 0:\n",
    "        robo1_response = \"I am sorry. Unable to understand you!\"\n",
    "    else:\n",
    "        robo1_response = sentence_tokens[idx]\n",
    "    \n",
    "    return robo1_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the chatflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I am a Learning Bot. Start typing your text. To end the conversation, type \"bye\"!\n",
      "Bot: there there!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\PC\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: what is a chatbot\n",
      "Bot: chatbot\n",
      "Bot: ai\n",
      "Bot: what is ai\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: goodbye\n",
      "Bot: dscsc\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n",
      "Bot: I am sorry. Unable to understand you!\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print('Hello! I am a Learning Bot. Start typing your text. To end the conversation, type \"bye\"!')\n",
    "\n",
    "while flag:\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    \n",
    "    if user_response != 'bye':\n",
    "        if user_response in ['thank you', 'thanks']:\n",
    "            flag = False\n",
    "            print('Bot: You are welcome...')\n",
    "        elif greet(user_response) is not None:\n",
    "            print('Bot:', greet(user_response))\n",
    "        else:\n",
    "            sentence_tokens.append(user_response)\n",
    "            word_tokens += nltk.word_tokenize(user_response)\n",
    "            print('Bot:', response(user_response))\n",
    "            sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print('Bot: Goodbye!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
